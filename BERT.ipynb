{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b74651c-6ef5-46f4-97e4-5d6d1160e8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training and evaluating on original data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8838\n",
      "Test Precision: 0.6689\n",
      "Test Recall: 0.6923\n",
      "Test F1 Score: 0.6804\n",
      "Training and evaluating on 30% test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9183\n",
      "Test Precision: 0.7647\n",
      "Test Recall: 0.7573\n",
      "Test F1 Score: 0.7610\n",
      "Training and evaluating on balanced data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9820\n",
      "Test Precision: 0.9695\n",
      "Test Recall: 0.9938\n",
      "Test F1 Score: 0.9815\n",
      "Training and evaluating on 30% test data for balanced_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9875\n",
      "Test Precision: 0.9819\n",
      "Test Recall: 0.9929\n",
      "Test F1 Score: 0.9874\n",
      "Training and evaluating on undersampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n",
      "Test Precision: 1.0000\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 1.0000\n",
      "Training and evaluating on 30% test data for undersampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n",
      "Test Precision: 1.0000\n",
      "Test Recall: 1.0000\n",
      "Test F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "class ArabicToxicDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item[0] for item in batch])\n",
    "    attention_mask = torch.stack([item[1] for item in batch])\n",
    "    labels = torch.tensor([item[2] for item in batch])\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "def preprocess_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['label'] = data['Majority_Label'].apply(lambda x: 1 if x == 'Offensive' else 0)\n",
    "    return data\n",
    "\n",
    "def oversample_data(data):\n",
    "    X = data['Comment']\n",
    "    y = data['label']\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X.values.reshape(-1, 1), y)\n",
    "    balanced_data = pd.DataFrame({\n",
    "        'Comment': X_resampled.flatten(),\n",
    "        'label': y_resampled\n",
    "    })\n",
    "    return balanced_data\n",
    "\n",
    "def undersample_data(data):\n",
    "    X = data['Comment']\n",
    "    y = data['label']\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_resampled, y_resampled = rus.fit_resample(X.values.reshape(-1, 1), y)\n",
    "    undersampled_data = pd.DataFrame({\n",
    "        'Comment': X_resampled.flatten(),\n",
    "        'label': y_resampled\n",
    "    })\n",
    "    # store undersampled_data in a csv file\n",
    "    undersampled_data.to_csv('undersampled_data.csv', index=False)\n",
    "    return undersampled_data\n",
    "\n",
    "def train_and_evaluate(data, model, tokenizer, device, epochs=5, batch_size=32):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_texts = train_data['Comment'].tolist()\n",
    "    train_labels = train_data['label'].tolist()\n",
    "    test_texts = test_data['Comment'].tolist()\n",
    "    test_labels = test_data['label'].tolist()\n",
    "\n",
    "    train_dataset = ArabicToxicDataset(train_texts, train_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    test_dataset = ArabicToxicDataset(test_texts, test_labels, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            # correct += (predictions == labels).sum().item()\n",
    "            # total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # accuracy = correct / total\n",
    "    # print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    # Calculate and print metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "def train_and_evaluate30(data, model, tokenizer, device, epochs=5, batch_size=32):\n",
    "    train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "\n",
    "    train_texts = train_data['Comment'].tolist()\n",
    "    train_labels = train_data['label'].tolist()\n",
    "    test_texts = test_data['Comment'].tolist()\n",
    "    test_labels = test_data['label'].tolist()\n",
    "\n",
    "    train_dataset = ArabicToxicDataset(train_texts, train_labels, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for input_ids, attention_mask, labels in train_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    test_dataset = ArabicToxicDataset(test_texts, test_labels, tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model.eval()\n",
    "    # correct = 0\n",
    "    # total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in test_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            # correct += (predictions == labels).sum().item()\n",
    "            # total += labels.size(0)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # accuracy = correct / total\n",
    "    # print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    # Calculate and print metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    data = preprocess_data('ardata.csv')\n",
    "\n",
    "    # Load the BERT model and tokenizer\n",
    "    model = BertForSequenceClassification.from_pretrained('asafaya/bert-base-arabic', num_labels=2)\n",
    "    tokenizer = BertTokenizer.from_pretrained('asafaya/bert-base-arabic')\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Train and evaluate on original data\n",
    "    print(\"Training and evaluating on original data...\")\n",
    "    train_and_evaluate(data, model, tokenizer, device)\n",
    "    #\n",
    "    # Train and evaluate on 30% test data\n",
    "    print(\"Training and evaluating on 30% test data...\")\n",
    "    train_and_evaluate30(data, model, tokenizer, device)\n",
    "    \n",
    "    # Oversample the data\n",
    "    balanced_data = oversample_data(data)\n",
    "    # Train and evaluate on oversampled data\n",
    "    print(\"Training and evaluating on balanced data...\")\n",
    "    train_and_evaluate(balanced_data, model, tokenizer, device)\n",
    "    #\n",
    "    # Train and evaluate on 30% test data\n",
    "    print(\"Training and evaluating on 30% test data for balanced_data...\")\n",
    "    train_and_evaluate30(balanced_data, model, tokenizer, device)\n",
    "\n",
    "    # Undersample the data\n",
    "    undersampled_data = undersample_data(data)\n",
    "\n",
    "    # Train and evaluate on undersampled data\n",
    "    print(\"Training and evaluating on undersampled data...\")\n",
    "    train_and_evaluate(undersampled_data, model, tokenizer, device)\n",
    "\n",
    "    # Train and evaluate on 30% test data for undersampled data\n",
    "    print(\"Training and evaluating on 30% test data for undersampled data...\")\n",
    "    train_and_evaluate30(undersampled_data, model, tokenizer, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c3c51c-4943-4b9e-9c79-28f9171b70b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5c4ba1-0658-4c6f-bda8-0924b444d08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training and evaluating on undersampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8037\n",
      "Test Precision: 0.8231\n",
      "Test Recall: 0.7810\n",
      "Test F1 Score: 0.8015\n",
      "Training and evaluating on 30% test data for undersampled data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8593\n",
      "Test Precision: 0.8565\n",
      "Test Recall: 0.8689\n",
      "Test F1 Score: 0.8627\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load and preprocess data\n",
    "    data = preprocess_data('ardata.csv')\n",
    "\n",
    "    # Load the BERT model and tokenizer\n",
    "    model = BertForSequenceClassification.from_pretrained('asafaya/bert-base-arabic', num_labels=2)\n",
    "    tokenizer = BertTokenizer.from_pretrained('asafaya/bert-base-arabic')\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "  \n",
    "    # Undersample the data\n",
    "    undersampled_data = undersample_data(data)\n",
    "\n",
    "    # Train and evaluate on undersampled data\n",
    "    print(\"Training and evaluating on undersampled data...\")\n",
    "    train_and_evaluate(undersampled_data, model, tokenizer, device)\n",
    "\n",
    "    # Train and evaluate on 30% test data for undersampled data\n",
    "    print(\"Training and evaluating on 30% test data for undersampled data...\")\n",
    "    train_and_evaluate30(undersampled_data, model, tokenizer, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92541e52-72b5-4068-8a59-b32966aa1605",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
